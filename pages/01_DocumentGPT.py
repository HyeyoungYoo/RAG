from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader 
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.memory import ConversationBufferMemory
from langchain.prompts import MessagesPlaceholder
import streamlit as st
import time

st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ˜",
)

def embed_file(file):
    st.write(file)
    #ì—…ë¡œë“œëœ íŒŒì¼ì„ ë¡œì»¬ì˜ ìºì‹œ í´ë”ì— ì €ì¥í•˜ì—¬ loaderë¡œ ë¶€ë¥¼ ìˆ˜ ìˆë„ë¡ í•œë‹¤.
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"

    with open(file_path,"wb") as f:  #writable, binaryë¡œ íŒŒì¼ ì—´ê¸°
        f.write(file_content) # í•´ë‹¹ íŒŒì¼ì— ë‚´ìš© ì“°ê¸°

    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    splitter = CharacterTextSplitter().from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,  
        chunk_overlap=100,
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs,cached_embeddings) 
    retriever = vectorstore.as_retriever()
    return retriever

st.title("DocumentGPT")
#ì‚¬ìš©ìì—ê²Œ íŒŒì¼ ì—…ë¡œë“œ ìš”ì²­

st.markdown("""
Welcome!
            
Use this chatbot to ask question to an AI about your files!
"""
)

file = st.file_uploader("Upload a .txt .pdf or .docx file", type=["txt","pdf","docx",]
)

if file:
   retriever = embed_file(file)
   s= retriever.invoke("winston")
   st.write(s)








