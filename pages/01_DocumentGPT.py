from email import message
from uuid import UUID
from langchain.document_loaders import UnstructuredFileLoader 
from langchain.schema.output import ChatGenerationChunk, GenerationChunk
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st
import time

st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ˜",
)

class ChatCallbackHandler(BaseCallbackHandler): #callback handelerëŠ” ê°ì¢… eventë“¤ì„ listení•˜ëŠ” functionë“¤ì„ ê°–ëŠ”ë‹¤.

    message = ""

    def on_llm_start(self, *args,**kwargs): #*argsì™€ **kwargë¡œ ìˆ˜ë§ì€ argument(1,2,3,...) ë° keyword argument(a=1,b=2,c=3...)ë¥¼ ë°›ì„ ìˆ˜ ìˆìŒ
        self.message_box = st.empty()  #ë¹ˆ ìœ„ì ¯ ì œê³µ

    def on_llm_end(self, *args,**kwargs):
        save_message(self.message,"ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)

llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,  #callback handlerë¡œ LLMì˜ ë‹µë³€ ê³¼ì •ì´ í™”ë©´ì— ë³´ì¼ ìˆ˜ ì¼ë„ë¡ í•¨.
                    #ChatOpenAIëŠ” ê°€ëŠ¥.ì¼ë¶€ ì˜¤ë˜ëœ ëª¨ë¸ì€ ì´ ê¸°ëŠ¥ì„ ì§€ì›X 
    callbacks=[
        ChatCallbackHandler(),
    ]
)

if "messages" not in st.session_state:
    st.session_state["messages"] = []

@st.cache_data(show_spinner="Embedding file...") #streamlitì˜ cache_dataí•¨ìˆ˜ëŠ” ì—…ë¡œë“œëœ íŒŒì¼ì´ ë™ì¼í•˜ë©´ ì´ í•¨ìˆ˜ë¥¼ ì¬ì‹¤í–‰í•˜ì§€ ì•Šê³  ê¸°ì¡´ì˜ ê°’ì„ ë‹¤ì‹œë°˜í™˜
def embed_file(file):
    #ì—…ë¡œë“œëœ íŒŒì¼ì„ ë¡œì»¬ì˜ ìºì‹œ í´ë”ì— ì €ì¥í•˜ì—¬ loaderë¡œ ë¶€ë¥¼ ìˆ˜ ìˆë„ë¡ í•œë‹¤.
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"

    with open(file_path,"wb") as f:  #writable, binaryë¡œ íŒŒì¼ ì—´ê¸°
        f.write(file_content) # í•´ë‹¹ íŒŒì¼ì— ë‚´ìš© ì“°ê¸°

    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}") #ì—…ë¡œë“œ íŒŒì¼ì´ë¦„ìœ¼ë¡œ ìºì‹œë””ë ‰í† ë¦¬ ë§Œë“¤ì–´
    splitter = CharacterTextSplitter().from_tiktoken_encoder( #spliter ìƒì„±
        separator="\n",
        chunk_size=600,  
        chunk_overlap=100,
    )
    loader = UnstructuredFileLoader(file_path) #ì—…ë¡œë“œëœ íŒŒì¼ì„ loaderë¡œ ê°€ì ¸ì™€
    docs = loader.load_and_split(text_splitter=splitter) #loaderì— ì˜¬ë ¤ì§„ íŒŒì¼ì„ splitterë¡œ ìª¼ê°œ []
    embeddings = OpenAIEmbeddings() #ì„ë² ë”©ìŠ¤ ìƒì„±
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir) # ì£¼ì–´ì§„ ë°”ì´íŠ¸ ì €ì¥ì†Œ(embeddings)ì—ì„œ ì„ë² ë”© ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬, ì´ë¥¼ ì§€ì •ëœ ìºì‹œ ë””ë ‰í„°ë¦¬(cache_dir)ì— ìºì‹±í•˜ëŠ” ê³¼ì •
    vectorstore = FAISS.from_documents(docs,cached_embeddings) #FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ(docs) ì»¬ë ‰ì…˜ìœ¼ë¡œë¶€í„° ë²¡í„° ì €ì¥ì†Œ(vectorstore)ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ì„¤ì •. docsëŠ” ë²¡í„°í™”í•˜ë ¤ëŠ” ë¬¸ì„œë“¤ì˜ ì»¬ë ‰ì…˜ì´ë©°, cached_embeddingsëŠ” ë¬¸ì„œë“¤ì„ ë²¡í„°í™”í•  ë•Œ ì‚¬ìš©í•  ì„ë² ë”© ë°ì´í„°. í•¨ìˆ˜ì˜ ê²°ê³¼ëŠ” ë²¡í„°ì˜ ì €ì¥ì†Œ vectorstoreë¼ëŠ” ë³€ìˆ˜ì— ì €ì¥
    retriever = vectorstore.as_retriever() #ì‚¬ìš©ìê°€ ìš”ì²­í•˜ëŠ” ì¿¼ë¦¬ì— ëŒ€í•´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë‚˜ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ vectorstoreì—ì„œ ê²€ìƒ‰í•´ ë°˜í™˜
    return retriever

def save_message(message, role):
    st.session_state["messages"].append({"message":message, "role":role})

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
       save_message(message, role)

#íˆìŠ¤í† ë¦¬ë¥¼ ê·¸ë ¤ë³´ëŠ” í•¨ìˆ˜
def paint_history():
    for message in st.session_state["messages"]:
        send_message(message["message"], message["role"], save=False,)

def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system",
         """
         Answer the question using ONLY the following context. If you don't know the answer just say you don't know, don't make anything up.

         Context: {context}
        """,
        ),
        ("human","{question}"),          
    ]  
)

st.title("DocumentGPT")

st.markdown("""
Welcome!
            
Use this chatbot to ask question to an AI about your files!
            
Upload your files on the sidebar!
"""
)
#ì‚¬ìš©ìì—ê²Œ íŒŒì¼ ì—…ë¡œë“œ ìš”ì²­ at sidebar
with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["txt","pdf","docx",]
    )

if file:
   retriever = embed_file(file)

   send_message("I'm ready! Ask away!","ai", save=False)
   paint_history()
   message = st.chat_input("Ask anything about your file...")
   if message:
        send_message(message,"human")
        chain = {                       
            "context":retriever | RunnableLambda(format_docs), 
            "question":RunnablePassthrough()
        } | prompt | llm
        #docs = retriever.invoke(message) #LangChainì€ chainì˜ inputì„ ì´ìš©í•´ retrieverë¥¼ ìë™ìœ¼ë¡œ invokeí•˜ë¯€ë¡œ í•„ìš”X
        with st.chat_message("ai"):
            chain.invoke(message)


else: 
    st.session_state["messages"] = [] #ì—…ë¡œë“œ íŒŒì¼ì´ ë°”ë€Œê±°ë‚˜ íŒŒì¼ì„ ì‚­ì œí•˜ë©´ ì €ì¥í–ˆë˜ ëŒ€í™” ì„¸ì…˜ ì´ˆê¸°í™”








